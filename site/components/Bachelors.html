<ContentCard
  title="Transforming Web Development with NLP and Vibe Coding Techniques (Joona Heinonen, 2025, BSc)"
  websiteLabel="Read about vibe coding"
  website="https://aaltodoc.aalto.fi/server/api/core/bitstreams/d8d8fb2d-6722-47cb-8b2c-35feda2436b6/content"
>
  <slot name="content"
    >With LLMs and AI getting significantly better in recent years, it has changed how web
development is approached. This thesis aims to cover the changes that vibe coding and
natural language programming bring to web development and what type of benefits
and drawbacks might emerge. Vibe coding and fully delegating development to AI
tools is a relatively new area in web development, which is the reason for this study.
This study is conducted as a literature review going over papers and research related
to vibe coding and no-code style programming in web development.
Web development has shifted from manually writing code to generating code and tests
with AI agents, which leaves resources for other parts of the process. Delegating
development to AI has several levels depending on the amount of work left to the
human developer, ranging from expert consultant to full delegation - in other words,
using vibe coding. Vibe coding is defined as leaving everything to the "vibes" referring
to fully letting the AI agent create and test the web application.
Vibe coding can bring benefits to the user with its speed and knowledge, offering an
easy way to try new projects and ideas with limited resources. it can also teach users
about certain algorithms and architectural decisions.
Applying vibe coding can also bring a series of drawbacks, such as vulnerabilities in the
form of security issues and risks. The use of LLMs can also bring ethical issues, such
as bias in the LLM outputs, LLM hallucinations in the form of non-factual outputs,
and the non-transparent use of AI. Furthermore, purely using AI to program can lead
to the decay of knowledge for the developer, leading to more risks in the future.
  </slot>
</ContentCard>
<ContentCard
  title="Systematic Literature Review of Agentic AI and AIOps Across Software Lifecycle (Eppu Ruotsalainen, 2025, BSc)"
  websiteLabel="Read about agentic AI"
  website="https://www.researchgate.net/publication/395711530_Systematic_Literature_Review_of_Agentic_AI_and_AIOps_Across_Software_Lifecycle"
>
  <slot name="content"
    >Agentic Artificial Intelligence (agentic AI) and AI for IT Operations (AIOps)
    are evolving concepts that bring autonomous decision-making agents into the
    Software Development Life Cycle (SDLC). This thesis conducted a systematic
    literature review of how these technologies are applied across the SDLC of
    software development. The review covers the design and coding phase, testing
    and Continuous Integration/Continuous Delivery (CI/CD), and operations and
    maintenance. The review followed PRISMA 2020 reporting principles. Relevant
    studies were identified from major digital libraries and screened against
    explicit inclusion criteria. Data were gathered and synthesized by SDLC
    phase to compare approaches, benefits, limitations and maturity, and to
    examine cross-phase patterns. The evidence reveals a clear progression of maturity.
    Operations is the most advanced: AIOps capabilities have been used in practice to
    reduce time-to-detect/resolve. Testing and CI/CD yield promising but uneven gains,
    constrained by trust, data availability and toolchain integration. Design and
    coding are the least mature — generative assistants speed up routine development,
    but still require human in the loop review. Overall, the integration of agentic
    AI across phases is still limited. This review highlights important research gaps,
    including the need for greater autonomous AI assistance in the early stages of
    development and stronger feedback loops that bring operational insights into
    testing and development through shared data models and interoperable tooling.
    In practice, organizations may gain the most by first using AIOps, then piloting
    AI-assisted testing and finally deploying coding assistants with strict guardrails.
    For researchers, priorities include early-phase agents with explainable outputs,
    cross-phase pipelines and longitudinal assessments of end-to-end impact.
  </slot>
</ContentCard>
<ContentCard
  title="A Comparative Study of the Specification Processes of ECMAScript and TypeScript (Can Kolho, 2025, BSc)"
  websiteLabel="Read about specification processes"
  website="https://www.researchgate.net/publication/393091345_A_Comparative_Study_of_the_Specification_Processes_of_ECMAScript_and_TypeScript"
>
  <slot name="content"
    >JavaScript was developed in 1995 to enhance interactivity on web pages, and
    today it is one of the most widely used programming languages in the world.
    Its official standard is ECMAScript, which is maintained by Technical
    Committee 39 (TC39) of ECMA International. Based on ECMAScript, Microsoft's
    open source project TypeScript includes all the features of JavaScript, with
    its most significant addition being static typing. This helps identify
    errors during development rather than at runtime. This thesis examines the
    development phases of ECMAScript and TypeScript before a feature is included
    in the official specification. The work was conducted as a literature
    review, with key sources including scientific articles, official ECMAScript
    specification, documentation maintained by the technical committee, as well
    as documents from Microsoft and the open source community. The aim of the
    research is to answer the question: How do the specification processes of
    ECMAScript and TypeScript differ from each other? The ECMAScript development
    is a well-defined six-stage standardization process. A proposal is advanced
    by a designated champion appointed by the ECMA organization and each stage
    requires approval from the TC39 commitee. In contrast to ECMAScript,
    TypeScript does not follow a formal specification process. Its development
    is more pragmatic and led by Microsoft. New features arise from practical
    needs without a structured phase model or committee process. This allows for
    a more responsive approach to the need of the developer community.
  </slot>
</ContentCard>
<ContentCard
  title="Local-First Software: Promises and Pitfalls (Otto Otsamo, 2025, BSc)"
  websiteLabel="Read about local-first software"
  website="https://www.researchgate.net/publication/392870932_Local-First_Software_Promises_and_Pitfalls"
>
  <slot name="content"
    >Software applications with collaboration features are typically built to
    store and process data on remote servers, making the client application
    completely dependent on those servers. This architectural pattern is simple
    to implement but has drawbacks: it is not possible to use the application
    without an internet connection, it is not always clear who owns the user's
    data, communication with the server may cause delays in actions taken by the
    user, and the application stops working if the service shuts down,
    potentially resulting in data loss. Local-first software refers to a
    software design pattern that prioritises storing and processing data locally
    on the user's device instead of remote servers. In this approach, servers
    are typically only utilised as a synchronisation tool to enable
    collaboration between users. This thesis reviews the benefits and drawbacks
    of each approach based on existing research and describes the common
    technologies typically used to build local-first software. The study
    demonstrates that the local-first pattern can be beneficial in applications
    involving asynchronous collaboration, but is unable to replace certain types
    of cloud-based software, such as those handling financial transactions or
    large datasets. Currently, the development of local-first applications
    remains relatively limited; however, there is clear interest in the topic
    within both academic and professional communities. To advance local-first
    development, it would be beneficial to research how existing synchronous
    data structures could be adapted to support asynchronous collaboration and
    how the memory overhead and other limitations of local-first data structures
    can be mitigated.
  </slot>
</ContentCard>
<ContentCard
  title="Comparison of web application architecture styles (Otto Söderman, 2025, BSc)"
  websiteLabel="Read about web application architectures"
  website="https://aaltodoc.aalto.fi/server/api/core/bitstreams/4e1121c4-6409-4032-91aa-381b7b17096b/content"
>
  <slot name="content"
    >Web application architecture styles define the overall structure of a web
    application. In today's web development landscape, there are numerous web
    architecture styles to choose from. Choosing the right architecture is often
    difficult, and the choice can have a significant impact on the performance,
    development, and maintainability of a given web software project. Currently,
    information about these architectures is scattered. Therefore, the purpose
    of this thesis is to consolidate this information. In particular, the
    research goal is to identify the commonly used web application architectures
    and map out their benefits and drawbacks. The research was conducted as a
    literature review, using conference proceedings, research articles, and
    books as the main sources. The four main software application architecture
    styles that were identified are: monolithic, tiered, microservices, and
    micro-frontends. Monolithic systems were found to be the easiest to setup
    and manage, with tiered systems being a bit harder, and microservice-based
    systems, which include micro-frontends, being even more difficult. However,
    microservice-based systems were found to often be easier to maintain and
    develop further features for, as opposed to monolithic and tiered systems.
    Microservice-based systems were also found to be more scalable than
    monoliths, while tiered systems sit somewhere in the middle. In addition,
    microservice-based systems, and to a lesser extent tiered systems, were less
    prone to technology lock-in, and more suitable for multi-team development
    than monolithic systems.
  </slot>
</ContentCard>
<ContentCard
  title="Comparison of the standardization processes of ECMAScript, HTML, and CSS (Juhana Laakso, 2025, BSc)"
  websiteLabel="Read about web standardization"
  website="https://www.researchgate.net/publication/391851352_Comparison_of_the_standardization_processes_of_ECMAScript_HTML_and_CSS"
>
  <slot name="content"
    >ECMAScript, HTML, and CSS are some of the main technologies of the
    Internet. Each of these languages is standardized using a different process
    and by a different organization. The standardization process of a
    programming language can impact how the language is developed and how widely
    it is used. Therefore, understanding and comparing standardization processes
    can help understand the current state and development of these languages.
    This thesis aims to describe the current standardization processes of the
    three languages in a comparable way and to identify the key similarities and
    differences of the processes by investigating the following research
    question through a literature review: What are the key similarities and
    differences in how ECMAScript, HTML, and CSS are standardized? Three key
    similarities in the standardization of the languages were identified: (1)
    global technology organizations have decision making power in the
    standardization organizations, (2) contributing is generally open to anyone,
    but decisions are made by appointed individuals, and (3) the standardization
    processes and decision making are publicly documented. In addition, four key
    differences were identified. (1) the processes and criteria for making
    changes are different for each standard, (2) decision making differs from
    the consensus based decisions of ECMAScript and CSS, to the editor based
    decisions of HTML, (3) the structure of the standards differs, from the
    single HTML standard document to the CSS modular standard, and (4) the
    publication cadences of the standards differ from the constantly updated
    HTML standard, to the annually updated ECMAScript standard. Further research
    could analyze the weaknesses and strengths of the three standardization
    processes in relation to each other.</slot
  >
</ContentCard>
<ContentCard
  title="Adapting SQLite to the Distributed Edge: A Comparative Study of Different Adaptations (Kalle Ahlström, 2025, BSc)"
  websiteLabel="Read about SQLite on distributed edge"
  website="https://aaltodoc.aalto.fi/server/api/core/bitstreams/e3df40d2-a6e3-4a6a-8341-d10837b2f834/content"
>
  <slot name="content"
    >This thesis is a comparative study of different SQLite adaptations and
    their viability for Distributed Edge Computing environments. The four
    adaptions chosen for closer analysis are FaaSFS, LiteFS, libSQL, and rqlite.
    An ideal architecture is presented and that is used as a basis for the
    comparison between the adaptations. None of the solutions studied implement
    the ideal architecture. Each has different trade-offs, making the selection
    process of what to use use-case specific. The main trade-offs highlight
    significant differences between write performance, stronger consistency
    guarantees, and/or increased read latency. According to the CAP theorem,
    distributed systems can’t fully guarantee consistency, availability, and
    partition tolerance simultaneously. Empirical analysis of the chosen
    adaptations indicates that implementation designs inherently favor either
    the CA or AP model, thereby substantiating the theorem.</slot
  >
</ContentCard>
<ContentCard
  title="Survey of web API definition languages (Niklas Saarikoski, 2025, BSc)"
  websiteLabel="Read about API definition languages"
  website="https://aaltodoc.aalto.fi/server/api/core/bitstreams/5689d6a8-f146-43d0-8313-21bfa5c4ee6c/content"
>
  <slot name="content"
    >Web Application Programming Interfaces (APIs) are the backbone of modern
    software development, enabling communication between diverse applications
    and platforms.To support the different needs, a variety of web API
    technologies have emerged, each with its own architecture, data transmission
    protocols, and design philosophies. This bachelor’s thesis introduces
    existing web APIs, examines applications developed for them, and evaluates
    their advantages and disadvantages. The comparison considers the popularity,
    efficiency, and architectural security of these APIs. The research has been
    conducted as a literature review, exploring existing materials to identify
    previous studies and potential gaps in the research. The comparison focuses
    on four web API technologies: RPC (Remote Procedure Call), SOAP (Simple
    Object Access Protocol), REST (Representational State Transfer), and GraphQL
    (Graph Query Language). These APIs represent the vast majority of web APIs
    in use today, and provide a good look in to the different limitations
    brought on with certain design choices Currently, the development of
    entirely new web API technologies seems unlikely. Although existing
    solutions have clear weaknesses, they are often offset by the strengths of
    other technologies. Nevertheless, in the future, new use cases might emerge
    that demand processes beyond the capabilities of current web API
    technologies.</slot
  >
</ContentCard>
<ContentCard
  title="Survey of Edge Databases for the Web (Arttu Pesonen, 2024, BSc)"
  websiteLabel="Read about edge databases"
  website="https://www.researchgate.net/publication/388001154_Survey_of_Edge_Databases_for_the_Web"
>
  <slot name="content"
    >Database as a Service offerings have transformed the development of web
    applications, enabling almost limitless scalability of databases according
    to the capacity and performance requirements. With the growth of global user
    bases in web applications, the use of traditional database systems can lead
    to high latency, as the database is often located far from the end users.
    Edge databases aim to bring data processing closer to the end users,
    reducing latency and improving service performance. While cloud computing
    has enabled the deployment of traditional databases in regions with the
    highest concentrations of end users, it has not fully addressed the
    challenges of growing global user bases. This thesis examines edge databases
    and their use in the context of web applications through a literature review
    and by comparing different edge database service providers. This study seeks
    to identify the edge databases designed for web applications and explore
    their features and design approaches. The technical review found that all
    the compared edge database solutions are compatible with existing database
    technologies and models: CockroachDB is compatible with PostgreSQL, Redis
    Cloud with Redis, and Turso with SQLite. This compatibility simplifies the
    adoption of the services, as developers can use familiar tools and
    programming languages, and integrate the services directly into existing web
    applications. All three compared edge database services enable extensive
    scalability by leveraging the infrastructure of major cloud service
    providers for deployment. Edge databases are especially beneficial for web
    applications that require precise management of database locations, as this
    has a significant impact on the latency experienced by end users. By using
    edge databases, web applications can improve their usability and customer
    experience by reducing service latency.</slot
  >
</ContentCard>
<ContentCard
  title="Comparison of web performance optimization techniques -1990s vs. 2020s (Klaus Rehnberg, 2024, BSc)"
  websiteLabel="Read about web performance optimization"
  website="https://www.researchgate.net/publication/387485302_Comparison_of_web_performance_optimization_techniques_-1990s_vs_2020s"
>
  <slot name="content"
    >The transition from web sites to web applications has led to numerous
    changes in web optimization methods. An understanding of the evolution of
    optimization methods is paramount for understanding why the current
    optimization methods are in use. In this thesis I compare web optimization
    methods used in the 1990s and methods used in modern web development. The
    methods I analyze in this thesis are limited to methods that have a
    measurable impact on user perceived latency, data throughput and resource
    usage of web servers. The literature survey made in this thesis shows that
    no previous works of this kind have been made from a high level overview. To
    keep itself on a high level, this thesis includes a multitude of sources
    covering multiple optimization methods. The results of the survey indicate
    that optimization methods have improved in multiple regards. Not only have
    existing methods improved, but new methods have been created to match the
    new requirements of web applications. Additionally, the focus of
    optimization methods has shifted from reducing the impact of low network
    bandwidth, to instead covering dynamic web data, which can be modified to
    suit a specific user, and the growing number of mobile devices, which have
    varying screen sizes and resolutions. Web servers have also moved from local
    servers to cloud and edge computing.</slot
  >
</ContentCard>
<ContentCard
  title="Survey of the current state of 3D production pipelines for the web (Lauri Lyytikäinen, 2024, BSc)"
  websiteLabel="Read about 3D production for the web"
  website="https://www.researchgate.net/publication/384687761_Survey_of_the_current_state_of_3D_production_pipelines_for_the_web"
>
  <slot name="content"
    >This paper aims to find out what is the current state of 3D production for
    the web. The research focuses on examining the process of creating 3D
    content, optimising it, and integrating it into a final product. It contains
    an overview of the long and complicated process of creating content for
    web-based platforms. The review was conducted as the current research on the
    topic is limited or the available information is several years old. The
    sources used were scientific articles and books about the topic, programming
    library documentation and programming articles. The research presents the
    different steps and tools available for creating good quality and functional
    3D content for web applications. The paper outlines how 3D data is created,
    edited, and exported and the different applications for it. How the
    different libraries, frameworks and tools can be used heavily depends on the
    final content’s specific purpose and use case, budget and target audience.
    The survey also explores the different criteria that the developers can use
    to select their libraries and pipelines, and when it might be better to
    create a custom implementation. The different applications of 3D content and
    their unique differences are also investigated. My main findings were that
    3D content in web applications is growing in popularity, and new uses for it
    are discovered continuously. While 3D technology has been around for
    multiple decades, it still is relatively new on the web platform. It is
    being avoided because of a complicated development process, technical
    limitations and the lack of standardisation. Different libraries and
    programming languages have greatly evolved in recent years and the standards
    in the industry are still forming.</slot
  >
</ContentCard>
<ContentCard
  title="Understanding re-emergence of the RPC model in web development (Ruupert Koponen, 2024, BSc)"
  websiteLabel="Read about the RPC model"
  website="https://www.researchgate.net/publication/380547826_Understanding_re-emergence_of_the_RPC_model_in_web_development"
>
  <slot name="content"
    >RPC is a request-response protocol from the 1980s, which allows developers
    to invoke remote functions from an application. It was originally used in
    distributed systems, but was also used in web development in the late 1990s
    and early 2000s. RPC's popularity began decreasing after the REST-style
    architecture appeared in the early 2000s. However, in the recent years, the
    RPC-model has re-emerged in web development. This bachelor's thesis focuses
    on understanding the recent growth in popularity of the RPC model. In
    addition, this work explores the impact of modern RPC frameworks on web
    development practices and offers comparison between REST, RPC and GraphQL.
    This thesis is a literature review that utilizes scientific articles,
    technological documentation and blog posts. This study found out that in the
    early 2000s, web development moved from RPC to REST architecture due to the
    low performance of RPC implementations at the time. The internet was growing
    rapidly, requiring scalable and high-performance web services, and REST
    answered to this demand. The 2010s saw new trends in web development, such
    as microservices and TypeScript. These trends led to the emergence of new
    RPC frameworks, such as gRPC and tRPC. These modern RPC frameworks leverage
    advancements in network protocols, serialization formats, and type systems
    to offer an alternative to the REST architectural style. gRPC is especially
    popular in microservices systems due to its high performance, whereas tRPC
    offers developers using TypeScript the ability to develop type-safe
    full-stack applications. The RPC model has also been integrated into the
    popular front-end framework React, which provides the ability to call
    server-side methods directly from client-side components. With the emergence
    of modern RPC frameworks, developers have more options in web service
    development tools. Developers have access to more specialized tools and can
    choose the one that best suits their needs. Although modern RPC
    implementations may not be as universally applicable as REST or GraphQL,
    they can offer significant benefits in certain types of applications. gRPC
    and tRPC streamline the development of web services by offering an improved
    developer experience. Additionally, they reduce the risk of errors through
    type safety.</slot
  >
</ContentCard>
<ContentCard
  title="Survey of Serverless Edge Computing for Web Applications (Mikael Siidorow, 2024, BSc)"
  websiteLabel="Read about serverless edge computing"
  website="https://www.researchgate.net/publication/380424331_Survey_of_Serverless_Edge_Computing_for_Web_Applications"
>
  <slot name="content"
    >Cloud computing has revolutionized the way we build software with
    infrastructure and computing power available always on demand. Serverless
    computing has allowed developers to focus on applications instead of
    infrastructure management. However, large data center are often far away
    from the end users, which introduces latency. Edge computing attempts to
    bring computing closer to the users, to the edge. Edge computing is already
    widely used in Internet of Things and mobile devices, and there exists a
    large amount of research this area. Web applications already use edge
    technologies such as Content Delivery Networks, but edge computing for
    dynamic computation is not yet widely used. This thesis reviews edge
    computing for web applications and summarizes research and service providers
    as literature research. The aim of the study is to determine what are the
    benefits and limitations of serverless edge computing compared to serverless
    cloud computing for web application development. The technical review showed
    that edge computing service providers were split in two based on the
    internal runtime: AWS uses micro virtual machines, while Cloudflare and Deno
    use V8 isolates. V8 isolates start faster with less overhead, but their
    technologies are more limited and security is less obvious than virtual
    machines. Serverless edge and cloud computing are not mutually exclusive
    technologies. The benefits of edge computing come from both closer location
    and faster runtimes. Edge is suitable for latency-critical parts of the
    application and V8 isolates significantly reduce load times. When developing
    serverless applications, some computation should be moved to the edge. Many
    web applications could improve usability and user experience by moving
    computation to the edge, reducing latency.</slot
  >
</ContentCard>
<ContentCard
  title="Using Storybook.js for component-driven design system web development (Tuomas Nummela, 2024, BSc)"
  websiteLabel="Read about Storybook.js"
  website="https://www.researchgate.net/publication/380396572_Using_Storybookjs_for_component-driven_design_system_web_development"
>
  <slot name="content"
    >Building web application user interfaces has become increasingly complex
    due to heightened expectations and requirements in interface
    customizability, visual appeal, performance, and accessibility. This has
    given birth to frontend tools like Storybook.js that help developers manage
    the development complexity through component isolation and test automation.
    Storybook is mainly used by developers to build and maintain design systems
    that can be used to construct UIs across applications regardless of the
    different application contexts. This thesis explores developer experiences
    with using Storybook to build and maintain design systems, identifying some
    benefits and drawbacks of the tool in the process. The developer experiences
    are inspected from the task viewpoints of designer-developer-handoff,
    component programming, and component testing. For each task, the Storybook
    way of approaching it is explained first. That is followed by a case study
    of using Storybook to help with that task in a software company. Finally,
    some observations are made from the experiences of using Storybook for that
    specific task. The final discussion presents and argues for four
    observations about Storybook. Firstly, Storybook offers approachable
    component workshopping. Secondly, Storybook doubles well as rich component
    documentation. Thirdly, there exist numerous add-ons to further leverage the
    use of Storybook. Finally, to fully leverage the capabilities of Storybook,
    developers must spend quite a lot of effort first configuring the tool and
    later creating and maintaining all of its content. The consensus of the
    developer experiences on building design systems using Storybook was
    positive.</slot
  >
</ContentCard>
<ContentCard
  title="Upcoming JavaScript web frameworks and their techniques (Juho Paakkanen, 2023, BSc)"
  websiteLabel="Read about upcoming JavaScript frameworks"
  website="https://www.researchgate.net/publication/376835044_Upcoming_JavaScript_web_frameworks_and_their_techniques"
>
  <slot name="content"
    >The interactivity and size of web applications have continuously increased,
    resulting in the fact that user interface frameworks alone often no longer
    meet the requirements of modern applications. To solve this, one of the
    latest trends in web software development has been using server-side
    rendering to improve the user experience. However, this has made the
    developer experience more complex, as a separate meta-framework must be
    included to achieve desired server-side functionalities. This thesis
    explores current mainstream and upcoming JavaScript web frameworks and their
    techniques by studying front-end frameworks and meta-frameworks. The
    research question the thesis aims to answer is: ”What benefits does the
    latest generation of JavaScript web frameworks offer over mainstream ones?”
    The goal is to find the best techniques for future web software development
    by analyzing research and observing practical trends. This is carried out
    through a literature review and by presenting the implementations of the
    selected frameworks. The frameworks selected for examination are React,
    Next, Svelte, Qwik, Astro, and HTMX. Many benefits of using upcoming
    frameworks and novel techniques were found. Newer solutions offer smaller
    runtime sizes, more efficient rendering approaches, and often a better
    developer experience. These are achieved by techniques such as more
    extensive compiler use and adopting finely-grained reactivity with signals.
    Most of this progress is incremental, and the benefits are mostly relevant
    for specific use cases. However, Qwik uses a technique called resumability,
    which might fundamentally change the currently prevalent server-side
    rendering by eliminating the need for hydration. Lastly, it is important to
    recognize that artificial intelligence will most likely have a significant
    impact on the future of web development.</slot
  >
</ContentCard>
<ContentCard
  title="Private Key Vulnerabilities in Browser Wallets (Jaakko Pentinsaari, 2023, BSc)"
  websiteLabel="Read about private key vulnerabilities"
  website="https://www.researchgate.net/publication/377397278_Private_Key_Vulnerabilities_in_Browser_Wallets"
>
  <slot name="content"
    >Blockchain technology and cryptocurrencies have gained significant
    attention both as a research topic as well as a platform for entertainment
    and finance services. The space has progressed from just currency to where
    it is now possible to also run a decentralized virtual machine that executes
    program code written on the blockchain. This decentralized virtual machine
    state allows developers to implement transactions and authentication that
    enable payment for services and proving ownership of digital assets.
    Implementing and using these technologies, however, has its own security
    concerns. One popular solution for users is a wallet that is conveniently
    stored in the browser and can easily interact with websites. This paper
    proposes how malicious extensions, screenshot attacks, keyloggers, XSS and
    malware could be used to steal private keys from the aforementioned browser
    wallets. It turns out that the secret recovery phrase protocol in popular
    browser wallet applications is particularly vulnerable and could be
    exploited with screenshots or keyloggers.</slot
  >
</ContentCard>
